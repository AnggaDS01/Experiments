{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_boston # For example\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ANN parameters\n",
    "n_hidden_layers = 2 # Number of hidden layers\n",
    "n_hidden_nodes = [64 ,32] # Number of hidden nodes for each hidden layer\n",
    "activation_function = 'relu' # Activation function for hidden nodes\n",
    "loss_function = 'mean_squared_error' # Loss function\n",
    "optimizer = 'adam' # Optimizer\n",
    "learning_rate = 0.001 # Learning rate\n",
    "batch_size = 32 # Batch size\n",
    "epoch = 100 # Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('./housing.data', delimiter=r\"\\s+\", header=None, names=column_names)\n",
    "\n",
    "X = data.drop(columns='MEDV')\n",
    "y = data['MEDV']\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to have values between 0 and 1\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "MSE: 14.769882054909692\n",
      "Training time: 6.53436279296875\n",
      "Prediction time: 0.13204193115234375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the architecture of the ANN model using keras Sequential class\n",
    "model = keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=X.shape[1])) # Input layer\n",
    "for i in range(n_hidden_layers):\n",
    "    model.add(layers.Dense(n_hidden_nodes[i] , activation=activation_function)) # Hidden layer\n",
    "model.add(layers.Dense(1)) # Output layer\n",
    "\n",
    "# Compile the ANN model with the specified loss function , optimizer , learning rate , and evaluation metric\n",
    "model.compile(loss=loss_function , optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the ANN model using keras fit method and measure the training time using time module\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train_scaled , y_train , batch_size=batch_size , epochs=epoch , validation_data=(X_test_scaled , y_test) , verbose=0)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Save the trained ANN model to a file or another source\n",
    "# model.save(\"ann_model.h5\")\n",
    "\n",
    "# Predict the outputs of the testing data using the trained ANN model and measure the prediction time using time module\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "end_time = time.time()\n",
    "prediction_time = end_time - start_time\n",
    "\n",
    "# Calculate the evaluation metric for the predictions of the testing data\n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "print(\"MSE:\", mse)\n",
    "# Print or plot the training time and prediction time\n",
    "print(\"Training time:\", training_time) \n",
    "print(\"Prediction time:\", prediction_time) \n",
    "plt.bar([\"Training\", \"Prediction\"], [training_time, prediction_time]) \n",
    "plt.xlabel(\"Process\") \n",
    "plt.ylabel(\"Time (s)\") \n",
    "plt.title(\"ANN time comparison\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load and preprocess data\n",
    "X = data.drop(columns='MEDV')\n",
    "y = data['MEDV'].values.reshape(-1,1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "X = tf.cast(scaler_X.fit_transform(X), tf.float32)\n",
    "# y = tf.cast(scaler_y.fit_transform(y), tf.float32)\n",
    "\n",
    "# # Split data into train and test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Train Loss: 3.4267, Test Loss: 110.0405\n",
      "Iteration 2, Train Loss: 3.2693, Test Loss: 219.6781\n",
      "Iteration 3, Train Loss: 3.0977, Test Loss: 162.7546\n",
      "Iteration 4, Train Loss: 2.9816, Test Loss: 135.7402\n",
      "Iteration 5, Train Loss: 2.8641, Test Loss: 169.6694\n",
      "Iteration 6, Train Loss: 2.8204, Test Loss: 148.1327\n",
      "Iteration 7, Train Loss: 2.8154, Test Loss: 310.3142\n",
      "Iteration 8, Train Loss: 2.7841, Test Loss: 279.9182\n",
      "Iteration 9, Train Loss: 2.7841, Test Loss: 158.1033\n",
      "Iteration 10, Train Loss: 2.7841, Test Loss: 147.4955\n",
      "Iteration 11, Train Loss: 2.7600, Test Loss: 234.4652\n",
      "Iteration 12, Train Loss: 2.7039, Test Loss: 240.7809\n",
      "Iteration 13, Train Loss: 2.6591, Test Loss: 180.0783\n",
      "Iteration 14, Train Loss: 2.6356, Test Loss: 137.1224\n",
      "Iteration 15, Train Loss: 2.6216, Test Loss: 136.7918\n",
      "Iteration 16, Train Loss: 2.6128, Test Loss: 155.9704\n",
      "Iteration 17, Train Loss: 2.5742, Test Loss: 139.9515\n",
      "Iteration 18, Train Loss: 2.5634, Test Loss: 141.3668\n",
      "Iteration 19, Train Loss: 2.5565, Test Loss: 135.0546\n",
      "Iteration 20, Train Loss: 2.5453, Test Loss: 133.2906\n",
      "Iteration 21, Train Loss: 2.5351, Test Loss: 134.4404\n",
      "Iteration 22, Train Loss: 2.5351, Test Loss: 134.4073\n",
      "Iteration 23, Train Loss: 2.5266, Test Loss: 139.5356\n",
      "Iteration 24, Train Loss: 2.5266, Test Loss: 139.2387\n",
      "Iteration 25, Train Loss: 2.5247, Test Loss: 141.7444\n",
      "Iteration 26, Train Loss: 2.5201, Test Loss: 140.4321\n",
      "Iteration 27, Train Loss: 2.5114, Test Loss: 138.7629\n",
      "Iteration 28, Train Loss: 2.4969, Test Loss: 144.0224\n",
      "Iteration 29, Train Loss: 2.4969, Test Loss: 147.0281\n",
      "Iteration 30, Train Loss: 2.4966, Test Loss: 144.6924\n",
      "Iteration 31, Train Loss: 2.4960, Test Loss: 142.0306\n",
      "Iteration 32, Train Loss: 2.4955, Test Loss: 143.4388\n",
      "Iteration 33, Train Loss: 2.4737, Test Loss: 149.4608\n",
      "Iteration 34, Train Loss: 2.4708, Test Loss: 146.0746\n",
      "Iteration 35, Train Loss: 2.4448, Test Loss: 141.4019\n",
      "Iteration 36, Train Loss: 2.4234, Test Loss: 147.3902\n",
      "Iteration 37, Train Loss: 2.4234, Test Loss: 159.6511\n",
      "Iteration 38, Train Loss: 2.4214, Test Loss: 160.7110\n",
      "Iteration 39, Train Loss: 2.4197, Test Loss: 144.6551\n",
      "Iteration 40, Train Loss: 2.4055, Test Loss: 149.6103\n",
      "Iteration 41, Train Loss: 2.4032, Test Loss: 149.7838\n",
      "Iteration 42, Train Loss: 2.3983, Test Loss: 147.7222\n",
      "Iteration 43, Train Loss: 2.3983, Test Loss: 158.3826\n",
      "Iteration 44, Train Loss: 2.3982, Test Loss: 166.2971\n",
      "Iteration 45, Train Loss: 2.3981, Test Loss: 152.0302\n",
      "Iteration 46, Train Loss: 2.3891, Test Loss: 153.8180\n",
      "Iteration 47, Train Loss: 2.3891, Test Loss: 156.7676\n",
      "Iteration 48, Train Loss: 2.3878, Test Loss: 161.1065\n",
      "Iteration 49, Train Loss: 2.3864, Test Loss: 153.1667\n",
      "Iteration 50, Train Loss: 2.3829, Test Loss: 142.2888\n",
      "Test RMSE: 11.9285\n",
      "Test MSE: 142.2889\n",
      "Test r_squared: -4.2918\n",
      "Test MAE: 8.8159\n"
     ]
    }
   ],
   "source": [
    "# Define ELM-CSO class\n",
    "class ELM_CSO:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        # Initialize parameters\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.input_weights = tf.Variable(tf.random.normal([n_input, n_hidden]), trainable=False)\n",
    "        self.output_weights = tf.Variable(tf.random.normal([n_hidden, n_output]), trainable=True)\n",
    "        self.bias = tf.Variable(tf.zeros([n_hidden]), trainable=False)\n",
    "        self.activation = tf.nn.relu\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Compute hidden layer output\n",
    "        H = self.activation(tf.matmul(X, self.input_weights) + self.bias)\n",
    "        # Compute output layer output\n",
    "        O = tf.matmul(H, self.output_weights)\n",
    "        return O\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, n_iter, n_pop):\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = tf.optimizers.Adam()\n",
    "        loss_fn = tf.losses.MeanSquaredError()\n",
    "\n",
    "        # Define CSO parameters\n",
    "        c1 = 1.49445 # cognitive parameter\n",
    "        c2 = 1.49445 # social parameter\n",
    "        w = 0.729 # inertia weight\n",
    "\n",
    "        # Initialize population and velocities\n",
    "        pop = tf.Variable(tf.random.uniform([n_pop, self.n_input * self.n_hidden + self.n_hidden], -1, 1))\n",
    "        vel = tf.Variable(tf.random.uniform([n_pop, self.n_input * self.n_hidden + self.n_hidden], -1, 1))\n",
    "\n",
    "        # Initialize personal best and global best positions and fitnesses\n",
    "        pbest_pos = tf.Variable(pop)\n",
    "        pbest_fit = tf.Variable(tf.ones([n_pop]) * np.inf)\n",
    "        gbest_pos = None\n",
    "        gbest_fit = np.inf\n",
    "\n",
    "        # Start iterations\n",
    "        for i in range(n_iter):\n",
    "            # Evaluate fitness of each particle\n",
    "            for j in range(n_pop):\n",
    "                # Decode particle position to input weights and bias\n",
    "                self.input_weights.assign(tf.reshape(pop[j][:self.n_input * self.n_hidden], [self.n_input, self.n_hidden]))\n",
    "                self.bias.assign(pop[j][self.n_input * self.n_hidden:])\n",
    "\n",
    "                # Train output weights with one-step learning\n",
    "                H_train = self.activation(tf.matmul(X_train, self.input_weights) + self.bias)\n",
    "                H_train_pinv = tf.linalg.pinv(H_train)\n",
    "                self.output_weights.assign(tf.matmul(H_train_pinv, y_train))\n",
    "\n",
    "                # Compute training loss\n",
    "                y_pred_train = self.forward(X_train)\n",
    "                loss_train = loss_fn(y_train, y_pred_train)\n",
    "\n",
    "                # Update personal best position and fitness if improved\n",
    "                if loss_train < pbest_fit[j]:\n",
    "                    pbest_pos[j].assign(pop[j])\n",
    "                    pbest_fit[j].assign(loss_train)\n",
    "\n",
    "                    # Update global best position and fitness if improved\n",
    "                    if loss_train < gbest_fit:\n",
    "                        gbest_pos = pop[j]\n",
    "                        gbest_fit = loss_train\n",
    "\n",
    "            # Update velocities and positions of each particle\n",
    "            for j in range(n_pop):\n",
    "                vel[j].assign(w * vel[j] + c1 * tf.random.uniform([1]) * (pbest_pos[j] - pop[j]) + c2 * tf.random.uniform([1]) * (gbest_pos - pop[j]))\n",
    "                pop[j].assign(pop[j] + vel[j])\n",
    "\n",
    "            # Compute test loss and print progress\n",
    "            y_pred_test = self.forward(X_test)\n",
    "            loss_test = loss_fn(y_test, y_pred_test)\n",
    "            print(f\"Iteration {i+1}, Train Loss: {gbest_fit.numpy():.4f}, Test Loss: {loss_test.numpy():.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict output given input\n",
    "        return self.forward(X)\n",
    "\n",
    "# Create ELM-CSO model with 10 hidden neurons\n",
    "model = ELM_CSO(n_input=13, n_hidden=150, n_output=1)\n",
    "\n",
    "# Train model with 100 iterations and 50 particles\n",
    "model.train(X_train, y_train, X_test, y_test, n_iter=50, n_pop=50)\n",
    "\n",
    "# Predict output for test set and compute RMSE\n",
    "y_pred_test_CSO_ELM = model.predict(X_test)\n",
    "# y_pred_test = scaler_y.inverse_transform(y_pred_test)\n",
    "# y_test = scaler_y.inverse_transform(y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test_CSO_ELM))\n",
    "mse_test = mean_squared_error(y_test, y_pred_test_CSO_ELM)\n",
    "r_squared_test = r2_score(y_test, y_pred_test_CSO_ELM)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test_CSO_ELM)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "print(f\"Test MSE: {mse_test:.4f}\")\n",
    "print(f\"Test r_squared: {r_squared_test:.4f}\")\n",
    "print(f\"Test MAE: {mae_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Train Loss: 0.5904, Test Loss: 0.8118\n",
      "Iteration 2, Train Loss: 0.5836, Test Loss: 0.8245\n",
      "Iteration 3, Train Loss: 0.5799, Test Loss: 0.7851\n",
      "Iteration 4, Train Loss: 0.5791, Test Loss: 0.7288\n",
      "Iteration 5, Train Loss: 0.5788, Test Loss: 0.7256\n",
      "Iteration 6, Train Loss: 0.5785, Test Loss: 0.7304\n",
      "Iteration 7, Train Loss: 0.5780, Test Loss: 0.7402\n",
      "Iteration 8, Train Loss: 0.5772, Test Loss: 0.7693\n",
      "Iteration 9, Train Loss: 0.5770, Test Loss: 0.7410\n",
      "Iteration 10, Train Loss: 0.5759, Test Loss: 0.7199\n",
      "Test Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "scaler_X = StandardScaler()\n",
    "encoder_y = OneHotEncoder()\n",
    "X = scaler_X.fit_transform(X)\n",
    "X = tf.cast(scaler_X.fit_transform(X), tf.float32)\n",
    "y = encoder_y.fit_transform(y).toarray()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define ELM-CSO class\n",
    "class ELM_CSO:\n",
    "    def __init__(self, n_input, n_hidden, n_output, activation, problem_type):\n",
    "        # Initialize parameters\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.input_weights = tf.Variable(tf.random.normal([n_input, n_hidden]), trainable=False)\n",
    "        self.output_weights = tf.Variable(tf.random.normal([n_hidden, n_output]), trainable=True)\n",
    "        self.bias = tf.Variable(tf.zeros([n_hidden]), trainable=False)\n",
    "        # Change activation function to tanh\n",
    "        self.activation = activation\n",
    "        self.problem_type = problem_type\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Compute hidden layer output\n",
    "        H = self.activation(tf.matmul(X, self.input_weights) + self.bias)\n",
    "        # Compute output layer output with conditional activation function\n",
    "        if self.problem_type == 'regression':\n",
    "            O = tf.matmul(H, self.output_weights)\n",
    "        elif self.problem_type == 'classification':\n",
    "            # Check if output neuron is 1 or more\n",
    "            if self.n_output == 1:\n",
    "                # Use sigmoid activation for binary classification\n",
    "                O = tf.nn.sigmoid(tf.matmul(H, self.output_weights))\n",
    "            else:\n",
    "                # Use softmax activation for multiclass classification\n",
    "                O = tf.nn.softmax(tf.matmul(H, self.output_weights))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid problem type. Must be either 'regression' or 'classification'.\")\n",
    "        return O\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, n_iter, n_pop, loss_function, optimizer):\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optimizer\n",
    "        # Change loss function to categorical crossentropy\n",
    "        loss_fn = loss_function\n",
    "\n",
    "        # Define CSO parameters\n",
    "        c1 = 1.49445 # cognitive parameter\n",
    "        c2 = 1.49445 # social parameter\n",
    "        w = 0.729 # inertia weight\n",
    "\n",
    "        # Initialize population and velocities as variables\n",
    "        pop = tf.Variable(tf.random.uniform([n_pop, self.n_input * self.n_hidden + self.n_hidden], -1, 1))\n",
    "        vel = tf.Variable(tf.random.uniform([n_pop, self.n_input * self.n_hidden + self.n_hidden], -1, 1))\n",
    "\n",
    "        # Initialize personal best and global best positions and fitnesses as variables\n",
    "        pbest_pos = tf.Variable(pop)\n",
    "        pbest_fit = tf.Variable(tf.ones([n_pop]) * np.inf)\n",
    "        gbest_pos = None\n",
    "        gbest_fit = np.inf\n",
    "\n",
    "        # Start iterations\n",
    "        for i in range(n_iter):\n",
    "            # Evaluate fitness of each particle\n",
    "            for j in range(n_pop):\n",
    "                # Decode particle position to input weights and bias\n",
    "                self.input_weights.assign(tf.reshape(pop[j][:self.n_input * self.n_hidden], [self.n_input, self.n_hidden]))\n",
    "                self.bias.assign(pop[j][self.n_input * self.n_hidden:])\n",
    "\n",
    "                # Train output weights with one-step learning\n",
    "                H_train = self.activation(tf.matmul(X_train, self.input_weights) + self.bias)\n",
    "                H_train_pinv = tf.linalg.pinv(H_train)\n",
    "                self.output_weights.assign(tf.matmul(H_train_pinv, y_train))\n",
    "\n",
    "                # Compute training loss\n",
    "                # Train output weights with gradient-based learning\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred_train = self.forward(X_train)\n",
    "                    loss_train = loss_fn(y_train, y_pred_train)\n",
    "                gradients = tape.gradient(loss_train, self.output_weights)\n",
    "                optimizer.apply_gradients(zip([gradients], [self.output_weights]))\n",
    "                # y_pred_train = self.forward(X_train)\n",
    "                # loss_train = loss_fn(y_train, y_pred_train)\n",
    "\n",
    "                # Update personal best position and fitness if improved\n",
    "                if loss_train < pbest_fit[j]:\n",
    "                    pbest_pos[j].assign(pop[j])\n",
    "                    pbest_fit[j].assign(loss_train)\n",
    "\n",
    "                    # Update global best position and fitness if improved\n",
    "                    if loss_train < gbest_fit:\n",
    "                        gbest_pos = pop[j]\n",
    "                        gbest_fit = loss_train\n",
    "\n",
    "            # Update velocities and positions of each particle\n",
    "            for j in range(n_pop):\n",
    "                vel[j].assign(w * vel[j] + c1 * tf.random.uniform([1]) * (pbest_pos[j] - pop[j]) + c2 * tf.random.uniform([1]) * (gbest_pos - pop[j]))\n",
    "                pop[j].assign(pop[j] + vel[j])\n",
    "\n",
    "            # Compute test loss and print progress\n",
    "            y_pred_test = self.forward(X_test)\n",
    "            loss_test = loss_fn(y_test, y_pred_test)\n",
    "            print(f\"Iteration {i+1}, Train Loss: {gbest_fit.numpy():.4f}, Test Loss: {loss_test.numpy():.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict output given input\n",
    "        return self.forward(X)\n",
    "\n",
    "# Create ELM-CSO model with 10 hidden neurons and 3 output neurons\n",
    "model = ELM_CSO(n_input=4, n_hidden=10, n_output=3, activation=tf.nn.relu, problem_type='classification')\n",
    "\n",
    "# Train model with 100 iterations and 50 particles\n",
    "model.train(X_train, y_train, X_test, y_test, n_iter=10, n_pop=510, loss_function=tf.losses.CategoricalCrossentropy(), optimizer=tf.optimizers.Adam())\n",
    "\n",
    "# Predict output for test set and compute accuracy score\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = np.argmax(y_pred_test.numpy(), axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "acc_test = accuracy_score(y_test,y_pred_test)\n",
    "print(f\"Test Accuracy: {acc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 3ms/step - loss: 666.7123\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 646.2822\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 625.1475\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 601.4096\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 571.9343\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 534.8645\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 486.3061\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 425.5171\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 352.6940\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 275.4070\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 203.3209\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 145.2121\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 108.8073\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 86.7638\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 70.0724\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 58.0177\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 49.6041\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43.4556\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 39.0478\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 35.9637\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 33.6118\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 32.1405\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 30.4502\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 29.4327\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 28.4846\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 27.7337\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 26.9574\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 26.2395\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 25.6068\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 25.0360\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.4963\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 24.0056\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 23.5315\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 23.0285\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 22.6272\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 22.1633\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.7284\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 21.4149\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 20.9034\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 20.5599\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 20.1433\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.7322\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.3884\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.1066\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 18.6966\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 18.3816\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 18.0149\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 17.7173\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 17.3780\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.0964\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.8308\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.5046\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.2760\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.9845\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.7188\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.4863\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.1870\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.0982\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.7406\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.5496\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.3171\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 14.0978\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.8977\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.6737\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.5166\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.2942\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.1333\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 13.1344\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 12.7616\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.6265\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.4664\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.3180\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.2511\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.0405\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.9387\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.8100\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.7118\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.5507\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.4044\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.3184\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.2641\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.0644\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.0717\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.9587\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.8410\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 10.6984\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.6707\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.5664\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.5753\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.4118\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3484\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.3032\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3400\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.1302\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.0174\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9818\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9300\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.8794\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.8032\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.7273\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Test RMSE: 4.0791\n",
      "Test MSE: 16.6389\n",
      "Test r_squared: 0.3812\n",
      "Test MAE: 3.1893\n"
     ]
    }
   ],
   "source": [
    "# # Define ANN model with 10 hidden neurons and 1 output neuron\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(33, activation='relu', input_shape=(13,)),\n",
    "#     tf.keras.layers.Dense(16, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# # Compile model with Adam optimizer and MSE loss function\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train model with 100 epochs and batch size of 32\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# # Predict output for test set and compute metrics\n",
    "# # Predict output for test set and compute RMSE\n",
    "# y_pred_test = model.predict(X_test)\n",
    "# # y_pred_test = scaler_y.inverse_transform(y_pred_test)\n",
    "# # y_test = scaler_y.inverse_transform(y_test)\n",
    "# rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "# mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "# r_squared_test = r2_score(y_test, y_pred_test)\n",
    "# mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "# print(f\"Test MSE: {mse_test:.4f}\")\n",
    "# print(f\"Test r_squared: {r_squared_test:.4f}\")\n",
    "# print(f\"Test MAE: {mae_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y test</th>\n",
       "      <th>y pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y test  y pred\n",
       "0        2       2\n",
       "1        2       2\n",
       "2        2       2\n",
       "3        2       2\n",
       "4        2       2\n",
       "5        2       2\n",
       "6        2       2\n",
       "7        2       2\n",
       "8        2       2\n",
       "9        2       2\n",
       "10       2       2\n",
       "11       2       2\n",
       "12       2       2\n",
       "13       2       1\n",
       "14       2       2\n",
       "15       2       2\n",
       "16       2       2\n",
       "17       2       2\n",
       "18       2       2\n",
       "19       2       2\n",
       "20       2       2\n",
       "21       2       2\n",
       "22       2       2\n",
       "23       2       2\n",
       "24       2       2\n",
       "25       2       2\n",
       "26       2       2\n",
       "27       2       2\n",
       "28       2       2\n",
       "29       2       2"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "y_test_df = pd.DataFrame(y_test, columns=['y test'])\n",
    "y_pred_df = pd.DataFrame(y_pred_test, columns=['y pred'])\n",
    "\n",
    "pd.concat([y_test_df, y_pred_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
